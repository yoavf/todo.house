"""AI provider interface and implementations."""

import json
import time
import uuid
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, Union
import logging
from pydantic import BaseModel, Field

from ..logging_config import AIProviderLogger

logger = logging.getLogger(__name__)


# Pydantic models for structured AI responses
class AITask(BaseModel):
    """Structure for a single task generated by AI."""

    title: str = Field(..., max_length=50, description="Clear, specific task title")
    description: str = Field(
        ..., description="Detailed description of what needs to be done"
    )
    priority: str = Field(
        ..., pattern="^(high|medium|low)$", description="Task priority level"
    )
    category: str = Field(
        ..., description="Task category (cleaning, repair, maintenance, safety, etc.)"
    )
    reasoning: str = Field(
        ..., description="Explanation of why this task was identified"
    )
    confidence: float = Field(
        ..., ge=0.0, le=1.0, description="Confidence score for this task (0.0-1.0)"
    )


class AIAnalysisResponse(BaseModel):
    """Structure for AI analysis response."""

    tasks: List[AITask] = Field(..., description="List of identified maintenance tasks")
    analysis_summary: str = Field(..., description="Overall summary of the analysis")


class AIProviderError(Exception):
    """Base exception for AI provider errors."""

    pass


class AIProviderRateLimitError(AIProviderError):
    """Exception raised when rate limit is exceeded."""

    def __init__(self, message: str, retry_after: Optional[int] = None):
        super().__init__(message)
        self.retry_after = retry_after


class AIProviderAPIError(AIProviderError):
    """Exception raised when AI provider API returns an error."""

    pass


class AIProvider(ABC):
    """Abstract base class for AI providers."""

    @abstractmethod
    async def analyze_image(self, image_data: bytes, prompt: str) -> Dict[str, Any]:
        """
        Analyze image and return structured response.

        Args:
            image_data: Raw image bytes
            prompt: Analysis prompt

        Returns:
            Dictionary containing analysis results with structure:
            {
                "analysis_summary": str,
                "tasks": List[Dict],
                "provider": str,
                "model": str,
                "processing_time": float,
                "tokens_used": Optional[int],
                "cost_estimate": Optional[float]
            }

        Raises:
            AIProviderError: If analysis fails
            AIProviderRateLimitError: If rate limit is exceeded
            AIProviderAPIError: If API returns an error
        """
        pass

    @abstractmethod
    def get_provider_name(self) -> str:
        """Return provider identifier."""
        pass

    @abstractmethod
    def get_usage_metrics(self) -> Dict[str, Any]:
        """
        Return usage statistics.

        Returns:
            Dictionary containing usage metrics:
            {
                "requests_made": int,
                "successful_requests": int,
                "failed_requests": int,
                "total_tokens_used": int,
                "total_cost_estimate": float,
                "average_response_time": float,
                "last_request_time": Optional[float]
            }
        """
        pass

    @abstractmethod
    def reset_usage_metrics(self) -> None:
        """Reset usage statistics."""
        pass


class GeminiProvider(AIProvider):
    """Google Gemini AI provider implementation."""

    def __init__(self, api_key: str, model: str = "gemini-1.5-flash"):
        """
        Initialize Gemini provider.

        Args:
            api_key: Google AI API key
            model: Model name to use

        Raises:
            ImportError: If google-generativeai is not installed
            ValueError: If API key is invalid
        """
        if not api_key:
            raise ValueError("API key is required for Gemini provider")

        self.api_key = api_key
        self.model = model
        self.provider_logger = AIProviderLogger("gemini")
        self._usage_metrics: Dict[
            str, Union[int, float, List[float], Optional[float]]
        ] = {
            "requests_made": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens_used": 0,
            "total_cost_estimate": 0.0,
            "response_times": [],
            "last_request_time": None,
        }

        # Import here to avoid dependency issues if not installed
        try:
            import google.generativeai as genai

            self.genai = genai
            genai.configure(api_key=api_key)
            self.client = genai.GenerativeModel(model)
        except ImportError as e:
            logger.error(f"Failed to import google.generativeai: {e}")
            raise ImportError(
                "google-generativeai is required for Gemini provider. "
                "Install it with: pip install google-generativeai"
            )

    async def analyze_image(self, image_data: bytes, prompt: str) -> Dict[str, Any]:
        """
        Analyze image using Gemini Vision API.

        Args:
            image_data: Raw image bytes
            prompt: Analysis prompt

        Returns:
            Dictionary containing analysis results

        Raises:
            AIProviderError: If analysis fails
            AIProviderRateLimitError: If rate limit is exceeded
            AIProviderAPIError: If API returns an error
        """
        request_id = str(uuid.uuid4())
        start_time = time.time()

        # Update request metrics
        requests_made = self._usage_metrics["requests_made"]
        if isinstance(requests_made, int):
            self._usage_metrics["requests_made"] = requests_made + 1
        self._usage_metrics["last_request_time"] = start_time

        # Log request start
        self.provider_logger.log_request(
            request_id=request_id,
            model=self.model,
            prompt_length=len(prompt),
            image_size=len(image_data),
        )

        try:
            # Create image part for Gemini
            image_part = {
                "mime_type": "image/jpeg",  # Assuming preprocessed images are JPEG
                "data": image_data,
            }

            # Generate content with image and prompt
            response = await self._make_api_call(prompt, image_part)

            # Parse response
            result = self._parse_response(response)

            # Calculate processing time
            processing_time = time.time() - start_time
            response_times = self._usage_metrics["response_times"]
            if isinstance(response_times, list):
                response_times.append(processing_time)

            # Update success metrics
            successful_requests = self._usage_metrics["successful_requests"]
            if isinstance(successful_requests, int):
                self._usage_metrics["successful_requests"] = successful_requests + 1

            if "tokens_used" in result:
                total_tokens = self._usage_metrics["total_tokens_used"]
                if isinstance(total_tokens, int):
                    self._usage_metrics["total_tokens_used"] = (
                        total_tokens + result.get("tokens_used", 0)
                    )

            if "cost_estimate" in result:
                total_cost = self._usage_metrics["total_cost_estimate"]
                if isinstance(total_cost, (int, float)):
                    self._usage_metrics["total_cost_estimate"] = float(
                        total_cost
                    ) + result.get("cost_estimate", 0.0)

            # Log successful response
            self.provider_logger.log_response(
                request_id=request_id,
                model=self.model,
                processing_time=processing_time,
                tokens_used=result.get("tokens_used"),
                cost_estimate=result.get("cost_estimate"),
                response_length=len(response.text) if hasattr(response, "text") else 0,
                tasks_parsed=len(result.get("tasks", [])),
            )

            # Add metadata to result
            result.update(
                {
                    "provider": self.get_provider_name(),
                    "model": self.model,
                    "processing_time": processing_time,
                    "request_id": request_id,
                }
            )

            return result

        except Exception as e:
            failed_requests = self._usage_metrics["failed_requests"]
            if isinstance(failed_requests, int):
                self._usage_metrics["failed_requests"] = failed_requests + 1

            processing_time = time.time() - start_time
            response_times = self._usage_metrics["response_times"]
            if isinstance(response_times, list):
                response_times.append(processing_time)

            # Handle specific error types and log them appropriately
            if "quota" in str(e).lower() or "rate limit" in str(e).lower():
                self.provider_logger.log_rate_limit(
                    request_id=request_id,
                    model=self.model,
                    retry_after=None,  # Could extract from error if available
                )
                logger.warning(f"Gemini rate limit exceeded: {e}")
                raise AIProviderRateLimitError(f"Rate limit exceeded: {e}")
            elif "api" in str(e).lower() or "invalid" in str(e).lower():
                self.provider_logger.log_error(
                    request_id=request_id,
                    model=self.model,
                    error_type="api_error",
                    error_message=str(e),
                    processing_time=processing_time,
                )
                logger.error(f"Gemini API error: {e}")
                raise AIProviderAPIError(f"API error: {e}")
            else:
                self.provider_logger.log_error(
                    request_id=request_id,
                    model=self.model,
                    error_type="unexpected_error",
                    error_message=str(e),
                    processing_time=processing_time,
                )
                logger.error(f"Unexpected Gemini error: {e}")
                raise AIProviderError(f"Unexpected error: {e}")

    async def _make_api_call(self, prompt: str, image_part: Dict[str, Any]) -> Any:
        """
        Make API call to Gemini.

        Args:
            prompt: Text prompt
            image_part: Image data part

        Returns:
            API response
        """
        try:
            # Prepare the image for Gemini
            from PIL import Image
            import io

            # Create PIL Image from bytes
            image = Image.open(io.BytesIO(image_part["data"]))

            # Configure generation for structured JSON output
            # Gemini supports a subset of JSON Schema
            response_schema = {
                "type": "object",
                "properties": {
                    "tasks": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "title": {"type": "string"},
                                "description": {"type": "string"},
                                "priority": {
                                    "type": "string",
                                    "enum": ["high", "medium", "low"],
                                },
                                "category": {"type": "string"},
                                "reasoning": {"type": "string"},
                                "confidence": {"type": "number"},
                                "task_types": {
                                    "type": "array",
                                    "items": {
                                        "type": "string",
                                        "enum": [
                                            "interior",
                                            "exterior",
                                            "electricity",
                                            "plumbing",
                                            "appliances",
                                            "maintenance",
                                            "repair",
                                        ],
                                    },
                                },
                            },
                            "required": [
                                "title",
                                "description",
                                "priority",
                                "category",
                                "reasoning",
                                "confidence",
                                "task_types",
                            ],
                        },
                    },
                    "analysis_summary": {"type": "string"},
                },
                "required": ["tasks", "analysis_summary"],
            }

            generation_config = self.genai.GenerationConfig(
                temperature=0.4,
                top_p=1,
                top_k=32,
                max_output_tokens=8192,
                response_mime_type="application/json",
                response_schema=response_schema,
            )

            # Update model configuration for this request
            model = self.genai.GenerativeModel(
                model_name=self.model,
                generation_config=generation_config,
            )

            # Generate content with the prompt and image
            response = model.generate_content([prompt, image])

            # Check if response was blocked
            if response.prompt_feedback and hasattr(
                response.prompt_feedback, "block_reason"
            ):
                logger.warning(f"Gemini blocked response: {response.prompt_feedback}")
                raise AIProviderAPIError("Content was blocked by Gemini safety filters")

            # Make sure we have candidates
            if not response.candidates:
                raise AIProviderAPIError("No response candidates from Gemini")

            return response

        except Exception as e:
            logger.error(f"Gemini API call failed: {e}")
            raise

    def _parse_response(self, response: Any) -> Dict[str, Any]:
        """
        Parse Gemini API response.

        Args:
            response: Raw API response

        Returns:
            Parsed response dictionary

        Raises:
            AIProviderError: If response parsing fails
        """
        try:
            # Extract text from response
            response_text = response.text

            # With structured output, we should get clean JSON
            try:
                parsed_data = json.loads(response_text)
                # Log the raw response to debug confidence scores
                logger.info(
                    f"Gemini raw response: {json.dumps(parsed_data, indent=2)[:1000]}..."
                )
            except json.JSONDecodeError as e:
                logger.error(
                    f"Failed to parse JSON from Gemini structured response: {e}"
                )
                logger.error(f"Response text: {response_text[:500]}...")
                raise AIProviderError(f"Invalid JSON response from Gemini: {e}")

            # Ensure required fields exist
            if "analysis_summary" not in parsed_data:
                parsed_data["analysis_summary"] = "No analysis summary provided"

            if "tasks" not in parsed_data:
                parsed_data["tasks"] = []

            # Validate task structure
            validated_tasks = []
            for task in parsed_data.get("tasks", []):
                if isinstance(task, dict) and "title" in task:
                    validated_task = {
                        "title": task.get("title", "Untitled task"),
                        "description": task.get(
                            "description", "No description provided"
                        ),
                        "priority": task.get("priority", "medium"),
                        "category": task.get("category", "general"),
                        "reasoning": task.get("reasoning", "No reasoning provided"),
                        "confidence": task.get("confidence", 0.5),
                    }
                    validated_tasks.append(validated_task)

            parsed_data["tasks"] = validated_tasks

            # Add token usage estimate
            # NOTE: The following token estimation logic is a placeholder and is overly simplistic.
            # It assumes an average of 4 characters per token, which may not accurately reflect
            # the actual tokenization rules of the language model being used. This should be
            # replaced with a proper token counting implementation using the tokenizer
            # associated with the specific model (e.g., OpenAI's tiktoken library).
            parsed_data["tokens_used"] = len(response_text) // 4  # Rough estimate
            parsed_data["cost_estimate"] = (
                parsed_data["tokens_used"] * 0.00001
            )  # Rough cost estimate

            return parsed_data

        except Exception as e:
            logger.error(f"Failed to parse Gemini response: {e}")
            raise AIProviderError(f"Response parsing failed: {e}")

    def get_provider_name(self) -> str:
        """Return provider identifier."""
        return "gemini"

    def get_usage_metrics(self) -> Dict[str, Any]:
        """Return usage statistics."""
        metrics = self._usage_metrics.copy()

        # Calculate average response time
        response_times = metrics["response_times"]
        if isinstance(response_times, list) and response_times:
            metrics["average_response_time"] = sum(response_times) / len(response_times)
        else:
            metrics["average_response_time"] = 0.0

        # Remove raw response times from output
        del metrics["response_times"]

        return metrics

    def reset_usage_metrics(self) -> None:
        """Reset usage statistics."""
        self._usage_metrics = {
            "requests_made": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens_used": 0,
            "total_cost_estimate": 0.0,
            "response_times": [],
            "last_request_time": None,
        }


class MockProvider(AIProvider):
    """Mock AI provider for testing and development."""

    def __init__(self, **kwargs):
        """Initialize mock provider."""
        self.provider_logger = AIProviderLogger("mock")
        self._usage_metrics: Dict[
            str, Union[int, float, List[float], Optional[float]]
        ] = {
            "requests_made": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens_used": 0,
            "total_cost_estimate": 0.0,
            "response_times": [],
            "last_request_time": None,
        }

        # Allow customization of mock responses
        self.mock_responses = kwargs.get(
            "mock_responses", self._get_default_responses()
        )
        self.response_index = 0
        self.cycle_responses = kwargs.get("cycle_responses", True)

    def _get_default_responses(self) -> List[Dict[str, Any]]:
        """Get default mock responses for variety."""
        return [
            {
                "tasks": [
                    {
                        "title": "Check HVAC filters",
                        "description": "Filters appear dirty and should be replaced",
                        "priority": "high",
                        "category": "maintenance",
                        "reasoning": "Dirty filters reduce efficiency and air quality",
                        "confidence": 0.9,
                        "task_types": ["interior", "maintenance"],
                    },
                    {
                        "title": "Inspect water heater",
                        "description": "Signs of rust on the water heater tank",
                        "priority": "medium",
                        "category": "maintenance",
                        "reasoning": "Early rust detection can prevent leaks",
                        "confidence": 0.75,
                        "task_types": ["interior", "plumbing", "maintenance"],
                    },
                ],
                "analysis_summary": "Found 2 maintenance items requiring attention",
            },
            {
                "tasks": [
                    {
                        "title": "Clean gutters",
                        "description": "Leaves and debris visible in gutters",
                        "priority": "medium",
                        "category": "maintenance",
                        "reasoning": "Clogged gutters can cause water damage",
                        "confidence": 0.65,
                        "task_types": ["exterior", "maintenance"],
                    }
                ],
                "analysis_summary": "Exterior maintenance needed",
            },
            {
                "tasks": [],
                "analysis_summary": "No immediate maintenance issues detected",
            },
        ]

    async def analyze_image(self, image_data: bytes, prompt: str) -> Dict[str, Any]:
        """Return mock analysis results."""
        request_id = str(uuid.uuid4())
        start_time = time.time()

        # Update metrics
        requests_made = self._usage_metrics["requests_made"]
        if isinstance(requests_made, int):
            self._usage_metrics["requests_made"] = requests_made + 1
        self._usage_metrics["last_request_time"] = start_time

        self.provider_logger.log_request(
            request_id=request_id,
            model="mock",
            prompt_length=len(prompt),
            image_size=len(image_data),
        )

        try:
            # Simulate processing delay
            import asyncio

            await asyncio.sleep(0.2)  # Simulate API latency

            # Get next mock response
            if self.cycle_responses:
                response_data = self.mock_responses[
                    self.response_index % len(self.mock_responses)
                ]
                self.response_index += 1
            else:
                response_data = self.mock_responses[0]

            # Deep copy to avoid modifying the template
            result = json.loads(json.dumps(response_data))

            # Add mock metadata
            processing_time = time.time() - start_time
            result.update(
                {
                    "provider": self.get_provider_name(),
                    "model": "mock",
                    "processing_time": processing_time,
                    "request_id": request_id,
                    "tokens_used": len(str(result)) // 4,  # Mock token count
                    "cost_estimate": 0.0,  # Free for mocking
                }
            )

            # Update success metrics
            successful_requests = self._usage_metrics["successful_requests"]
            if isinstance(successful_requests, int):
                self._usage_metrics["successful_requests"] = successful_requests + 1

            response_times = self._usage_metrics["response_times"]
            if isinstance(response_times, list):
                response_times.append(processing_time)

            self.provider_logger.log_response(
                request_id=request_id,
                model="mock",
                processing_time=processing_time,
                tokens_used=result.get("tokens_used"),
                cost_estimate=0.0,
                response_length=len(str(result)),
                tasks_parsed=len(result.get("tasks", [])),
            )

            return result

        except Exception as e:
            failed_requests = self._usage_metrics["failed_requests"]
            if isinstance(failed_requests, int):
                self._usage_metrics["failed_requests"] = failed_requests + 1

            processing_time = time.time() - start_time
            self.provider_logger.log_error(
                request_id=request_id,
                model="mock",
                error_type="mock_error",
                error_message=str(e),
                processing_time=processing_time,
            )

            logger.error(f"Mock provider error: {e}")
            raise AIProviderError(f"Mock provider error: {e}")

    def get_provider_name(self) -> str:
        """Return provider identifier."""
        return "mock"

    def get_usage_metrics(self) -> Dict[str, Any]:
        """Return usage statistics."""
        metrics = self._usage_metrics.copy()

        # Calculate average response time
        response_times = metrics["response_times"]
        if isinstance(response_times, list) and response_times:
            metrics["average_response_time"] = sum(response_times) / len(response_times)
        else:
            metrics["average_response_time"] = 0.0

        # Remove raw response times from output
        del metrics["response_times"]

        return metrics

    def reset_usage_metrics(self) -> None:
        """Reset usage statistics."""
        self._usage_metrics = {
            "requests_made": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens_used": 0,
            "total_cost_estimate": 0.0,
            "response_times": [],
            "last_request_time": None,
        }


class AIProviderFactory:
    """Factory for creating AI provider instances."""

    _supported_providers = {"gemini": GeminiProvider, "mock": MockProvider}

    @classmethod
    def create_provider(cls, provider_name: str, **kwargs) -> AIProvider:
        """
        Create AI provider instance.

        Args:
            provider_name: Name of the provider to create
            **kwargs: Provider-specific configuration

        Returns:
            AIProvider instance

        Raises:
            ValueError: If provider name is not supported
        """
        provider_name = provider_name.lower()

        if provider_name not in cls._supported_providers:
            supported = ", ".join(cls._supported_providers.keys())
            raise ValueError(
                f"Unsupported AI provider: {provider_name}. "
                f"Supported providers: {supported}"
            )

        provider_class = cls._supported_providers[provider_name]

        try:
            return provider_class(**kwargs)
        except Exception as e:
            logger.error(f"Failed to create {provider_name} provider: {e}")
            raise ValueError(f"Failed to create {provider_name} provider: {e}")

    @classmethod
    def get_supported_providers(cls) -> list[str]:
        """
        Get list of supported provider names.

        Returns:
            List of supported provider names
        """
        return list(cls._supported_providers.keys())

    @classmethod
    def register_provider(cls, name: str, provider_class: type[AIProvider]) -> None:
        """
        Register a new AI provider.

        Args:
            name: Provider name
            provider_class: Provider class that implements AIProvider

        Raises:
            ValueError: If provider class doesn't implement AIProvider
        """
        if not issubclass(provider_class, AIProvider):
            raise ValueError("Provider class must implement AIProvider interface")

        cls._supported_providers[name.lower()] = provider_class  # type: ignore
        logger.info(f"Registered AI provider: {name}")
